# -*- coding: utf-8 -*-
"""Models.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EDch6-ApCVubIOAWEafNNiLL0wm37ES-
"""



import pandas as pd
import spacy
import numpy as np

!python -m spacy download en_vectors_web_lg
!python -m spacy link en_vectors_web_lg en_vectors_web_lg

from google.colab import auth
auth.authenticate_user()
import gspread
from oauth2client.client import GoogleCredentials
gc = gspread.authorize(GoogleCredentials.get_application_default())

"""# General idea

We will be using spacy to get the nlp 300 dimensional vectorization of the sentences/questions.

We will use this as the covariant matrix for are model.

The matrix will be of dimension n by 300.
We will convert labels into a categorical data type. This will be our "y" and it will be of length n as well.

Train/test split. HOLD OFF TRAINING VALIDATION TO THE VERY VERY END WHEN WE ARE PRETTY MUCH ALREADY CERTAIN THAT IT WILL WORK.

We will use this to train a SVM model with k-fold cross validation.

# Lets get started!

Loading the data
"""

source = 'https://docs.google.com/spreadsheets/d/1CH25lwZMn9o9HYgkFOSeQRUUtmsvTZd801vmQkAi1BY/edit?usp=sharing'
workbook = gc.open_by_url(source)
sheet = workbook.sheet1
data = sheet.get_all_values()

sample_data = pd.DataFrame(data)
sample_data.columns = sample_data.iloc[0]
sample_data = sample_data.iloc[1:]
display(sample_data.head())

sample_data = sample_data[["Question", "Large Category"]]
sample_data

from sklearn.model_selection import train_test_split

df_train, df_test = train_test_split(sample_data, test_size=0.2, random_state=42)
len(df_train), len(df_test)

"""We need to create a pipeline.

1. Transform X to vectors
2. Transform Y to categorical
3. Fit SVM
"""

# Create a pipeline step for spacy
# from sklearn.preprocessing import LabelEncoder

from sklearn.pipeline import Pipeline
from sklearn.base import TransformerMixin
from sklearn.decomposition import PCA
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression

class TextVectorizer(TransformerMixin):
  def transform(self, X, **transform_params):
    nlp = spacy.load('en_vectors_web_lg')
    new_X = np.zeros((len(X), nlp.vocab.vectors_length))
    # Iterate over the sentences
    for idx, sentence in enumerate(X):
        # Pass each sentence to the nlp object to create a document
        doc = nlp(sentence)
        # Save the document's .vector attribute to the corresponding row in     
        # X
        new_X[idx, :] = doc.vector
    return new_X
  def fit(self, X, y=None, **fit_params):
    return self

pipe = Pipeline([("Spacy_Vectorizer", TextVectorizer()),
                 ('PCA', PCA(n_components=10)),
                 ('Classifier',  LogisticRegression())])

# Support Vector Classifier
# Pretrained models in things like pytorch
# Perhaps build a model that generates sentences from the data?
# Scree plot for PCA?
# k-fold cross validation for n_components
# smaller nlp model for speed?

pipe.fit(df_train["Question"], df_train["Large Category"])

pipe.predict(df_train["Question"])[:10]

from sklearn.metrics import confusion_matrix
from sklearn.metrics import plot_confusion_matrix
import matplotlib.pyplot as plt

fig, ax = plt.subplots(figsize=(10, 10))
plot_confusion_matrix(pipe, df_train.Question, df_train["Large Category"], normalize='true', ax=ax);

df_train["Large Category"].value_counts()

# Remove Junk and Local Fire Information
# Try to move stuff from Other
# Health is probably important and something we should keep if we can.

len(df_train)

pipe.predict_proba(df_train["Question"].iloc[0:1])

plt.hist([max(prediction) for prediction in pipe.predict_proba(df_train["Question"])], density=True, bins=np.arange(0,1.05,.025));

from sklearn.model_selection import cross_val_score

scores = cross_val_score(pipe, X=df_train.Question, y=df_train["Large Category"], cv=5)

np.mean(scores)

